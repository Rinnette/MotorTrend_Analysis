---
title: "Determining whether automatic or manual transmission is better for MPG"
author: "Rinnette N. Ramdhanie"
date: "19 December 2019"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r addLibrary, warning = FALSE, message = FALSE, echo = FALSE}
        library(dplyr); library(ggplot2); library(caret); library(olsrr)
```

```{r setupData, echo = FALSE}
        data(mtcars); m <- mtcars
```

## Executive Summary
The *Motor Trend* data in the R datasets package was analysed to explore the relationship between miles per gallon (MPG), the outcome, and type of transmission, automatic or manual.  Several models were done using multiple regression and it was shown that manual cars are better for MPG than automatics cars.  The analysis also showed that number of cylinders, horsepower and weight had an effect on the MPG.

## Exploratory data analysis
The Motor Trend dataset used is the *mtcars* dataset which consists of information on a collection of cars.

The structure of the dataset and the number of vehicles with each transmission type were checked, as well as the average mpg for each type of transmission was calculated.  Note that in the *am* variable, 0 represents automatic transmission and 1 represents manual transmission.  
\scriptsize
```{r explData, echo = FALSE}
        str(m); table(m$am); aggregate(mpg ~ am, m, mean)
```
\normalsize

A boxplot of the data was done on MPG by transmission type and this showed that there were no outliers and that generally manual cars get higher MPG than automatic cars.  This figure can be found in the Appendix.

## Model selection and strategy
An initial model was fitted using linear regression with *mpg* as the outcome and *am* as the predictor variable.  Note that *am* was first converted to a factor variable (0=automatic, 1=manual).  The adjusted R-squared value of this initial model was checked to determine how much of the variance is explained by the variable *am*.  If it is too low then another predictor was added to the model and the adjusted R-squared value was checked again.  This process was repeated until the value reached a maximum.

To decide on which variables to include as predictors and the order in which to add them, the correlation coefficients between MPG and the other variables in the dataset were calculated. Those with high correlation were considered.

Resulting models were then checked using anova to see how necessary it was to add each predictor.  A final model was then determined.

### Fitting the models
An initial model was fitted with *mpg* as the outcome and *am* the predictor.  Only about 34% of the variation in the *mpg* is explained by the variation in *am*. This is very low so we add other predictors to the model.  Code and summary for the model can be found in the Appendix.  The correlation coefficients were calculated to see how *mpg* varies with the other variables in the dataset.

The variables *cyl*, *wt*, *disp*, *hp*, *drat* and *vs* showed the highest correlation in that order.  As a result, each of these variables were added to the model in turn and the adjusted R-squared values compared after each addition.
\scriptsize
```{r buildModels, echo = FALSE}
        # Build models in stepwise fashion
                m$am = factor(m$am)
                m$cyl <- factor(m$cyl)
                m$vs <- factor(m$vs)
                m1 <- lm(mpg ~ am, m)
                m2 <- update(m1, .~. + cyl)
                m3 <- update(m2, .~. + wt)
                m4 <- update(m3, .~. + disp)
                m5 <- update(m4, .~. + hp)
                m6 <- update(m5, .~. + drat)
                m7 <- update(m6, .~. + vs)
        
        # Create data frame of adjusted r-squared values for each model
                mnames <- c("m1", "m2", "m3", "m4", "m5", "m6", "m7")
                predictors <- c("am", "am+cyl", "am+cyl+wt", "am+cyl+wt+disp", "am+cyl+wt+disp+hp", "am+cyl+wt+disp+hp+drat", "am+cyl+wt+disp+hp+drat+vs")
                mAR2s <- c(summary(m1)$adj.r.squared, summary(m2)$adj.r.squared, 
                   summary(m3)$adj.r.squared, summary(m4)$adj.r.squared, 
                   summary(m5)$adj.r.squared, summary(m6)$adj.r.squared, 
                   summary(m7)$adj.r.squared)
                data.frame(Model = mnames, Predictors = predictors, AdjRSquared = mAR2s)
```
\normalsize
The table shows figures for the adjusted R-squared values for each model.  The values increase after each predictor is added up until *drat* was added (m6) .  Model **m5** therefore showed the highest adjusted r-squared of about 83% and was a possible final model.

The *anova* function was then used to check whether the inclusion of each predictor was necessary. The code and results can be found in the Appendix.  Looking at the p-values for the associated F statistic, it was seen that the inclusion of *cyl*, *wt* and *hp* appeared to be necessary since these p-values were less than 0.05.  The inclusion of the *disp* predictor however, appeared to have been unnecessary as its p-value of 0.89 is greater than 0.05. The *disp* predictor was therefore excluded from the final model.
\scriptsize
```{r fitFinal}
        mfin <- lm(mpg ~ am + cyl + wt + hp, m)
```
\normalsize
In the final model, the adjusted R-squared value is now about 84%.

## Residuals and Diagnostics
In the Residuals vs Fitted plot, the residuals are more or less linear on the zero line with a few outliers.

In the Normal Q-Q plot, the points are almost all along the diagonal therefore this shows that the residuals are approximately normally distributed. A couple outliers can also be seen here.  Both plots can be found in the Appendix.

To determine how influential these outliers were to the prediction model, some regression diagnostics were done using the dffits and rstudent functions. These plots are in the Appendix.  All vaues in the Studentized Residuals plot were within the thresholds while in the dffits plot, there were a few values which were just beyond the thresholds, but not by much.  If the model was adjusted to cater for these we run the risk of overfitting as they were close to the thresholds.

## Results
The summary of the final model is available in the Appendix.  It shows the p-value is $1.506 \times 10^{-10}$, which is about zero, and means that the model describes a statistically significant relationship.

The coefficient for *am1* shows that manual transmission is better for MPG than automatic transmission and is associated with an average increase of 1.8 MPG.

\pagebreak
## Appendix

### Exploratory data analysis
Looking at the dataset.
\scriptsize
```{r explDat, results = "hide"}
        str(m); table(m$am); aggregate(mpg ~ am, m, mean)
```
\normalsize


Boxplot on MPG and transmission.
\scriptsize
```{r boxplotAM, fig.width=5, fig.height=2}
        m <- mutate(m, Transmission = factor(m$am))
        levels(m$Transmission) = c("Automatic", "Manual")
        ggplot(m, aes(x=Transmission, y=mpg, fill = Transmission)) +
          geom_boxplot()
```
\normalsize

### Fitting the model
Fit the initial model with *am* as the predictor.
\scriptsize
```{r fitInitModel}
        m$am = factor(m$am)
        m1 <- lm(mpg ~ am, m)
        summary(m1)$adj.r.squared
```
\normalsize

Calculate correlation between MPG and all other variables.
\scriptsize
```{r checkCorr}
        cor(mtcars$mpg, mtcars)
```
\normalsize

Fit other models.
\scriptsize
```{r buildMods, results="hide"}
        # Build models in stepwise fashion
                m$am = factor(m$am)
                m$cyl <- factor(m$cyl)
                m$vs <- factor(m$vs)
                m1 <- lm(mpg ~ am, m)
                m2 <- update(m1, .~. + cyl)
                m3 <- update(m2, .~. + wt)
                m4 <- update(m3, .~. + disp)
                m5 <- update(m4, .~. + hp)
                m6 <- update(m5, .~. + drat)
                m7 <- update(m6, .~. + vs)
        
        # Create data frame of adjusted r-squared values for each model
                mnames <- c("m1", "m2", "m3", "m4", "m5", "m6", "m7")
                predictors <- c("am", "am+cyl", "am+cyl+wt", "am+cyl+wt+disp", 
                                "am+cyl+wt+disp+hp", "am+cyl+wt+disp+hp+drat", 
                                "am+cyl+wt+disp+hp+drat+vs")
                mAR2s <- c(summary(m1)$adj.r.squared, summary(m2)$adj.r.squared, 
                   summary(m3)$adj.r.squared, summary(m4)$adj.r.squared, 
                   summary(m5)$adj.r.squared, summary(m6)$adj.r.squared, 
                   summary(m7)$adj.r.squared)
                data.frame(Model = mnames, Predictors = predictors, AdjRSquared = mAR2s)
```
\normalsize

Use ANOVA to check the necessity of the inclusion of each predictor.
\scriptsize
```{r anovaCheck}
        anova(m1, m2, m3, m4, m5)
```
\normalsize

### Residuals & Diagnostics
Residual plots.
\scriptsize
```{r plotResid, fig.width = 6, fig.height = 3.5}
        ## Plot Residuals vs Fitted and Normal Q-Q plots
                par(mfrow = c(1,2))        
                plot(mfin, which = 1)
                plot(mfin, which = 2)
```
\normalsize

Plot diagnostics.
\scriptsize
```{r diagPlots, fig.width = 5, fig.height = 3.5}
        # Plot dffits and rstudent diagnostic values         
                ols_plot_dffits(mfin)
                ols_plot_resid_stud(mfin)
```
\normalsize

